# -*- coding: utf-8 -*-
"""major_model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P_g6uccqpAxASHW8FbXTPY3InKAZnv7O
"""

!pip install pandas scikit-learn numpy joblib nltk python-docx PyPDF2 imbalanced-learn sentence-transformers

import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

df = pd.read_csv("resume_dataset.csv")   # replace with your dataset file
print("Columns in dataset:", df.columns)
df.head()

X = df["cleaned_text"]   # input feature
y = df["Domains"]        # target label

print("Sample text:", X.iloc[0])
print("Sample label:", y.iloc[0])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=None
)

print("Training size:", len(X_train))
print("Testing size:", len(X_test))

vectorizer = TfidfVectorizer(max_features=5000, stop_words="english", ngram_range=(1,2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print("Shape of TF-IDF matrix:", X_train_tfidf.shape)

model = LogisticRegression(max_iter=500, class_weight="balanced")
model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

joblib.dump(model, "domain_classifier.pkl")
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

print("âœ… Model and vectorizer saved successfully!")

import PyPDF2
import docx
from google.colab import files
import joblib

model = joblib.load("domain_classifier.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")

# Load dataset to get skills mapping
import pandas as pd
df = pd.read_csv("resume_dataset.csv")   # same dataset used for training

def read_pdf(file_path):
    pdf_reader = PyPDF2.PdfReader(file_path)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() + " "
    return text

def read_docx(file_path):
    doc = docx.Document(file_path)
    return " ".join([para.text for para in doc.paragraphs])

def read_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    text = " ".join([word for word in text.split() if word not in stop_words])
    return text

def predict_resume(file_path, file_type):
    # Read file
    if file_type == "pdf":
        text = read_pdf(file_path)
    elif file_type == "docx":
        text = read_docx(file_path)
    elif file_type == "txt":
        text = read_txt(file_path)
    else:
        return "Unsupported file type"

    # Clean text
    text_clean = clean_text(text)

    # Vectorize
    vect = vectorizer.transform([text_clean])

    # Predict domain
    pred_domain = model.predict(vect)[0]

    # Confidence score (based on probability)
    if hasattr(model, "predict_proba"):
        prob = model.predict_proba(vect).max() * 10  # scale to 10
    else:
        prob = 10  # for models without predict_proba

    # Relevant skills
    skills = df[df["Domains"] == pred_domain]["Skills"].values[0]

    return pred_domain, round(prob,2), skills

uploaded = files.upload()

for filename in uploaded.keys():
    file_type = filename.split('.')[-1].lower()
    domain, rating, skills = predict_resume(filename, file_type)
    print(f"File: {filename}")
    print(f"Predicted Domain: {domain}")
    print(f"Relevant Skills: {skills}\n")